{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Flower 102 and visualisation"
      ],
      "metadata": {
        "id": "LHjaa_7FVXO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(x,title=None):\n",
        "    # Move tensor to CPU and convert to numpy\n",
        "    x_np = x.cpu().numpy()\n",
        "\n",
        "    # If tensor is in (C, H, W) format, transpose to (H, W, C)\n",
        "    if x_np.shape[0] == 3 or x_np.shape[0] == 1:\n",
        "        x_np = x_np.transpose(1, 2, 0)\n",
        "\n",
        "    # If grayscale, squeeze the color channel\n",
        "    if x_np.shape[2] == 1:\n",
        "        x_np = x_np.squeeze(2)\n",
        "\n",
        "    x_np = x_np.clip(0, 1)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    if len(x_np.shape) == 2:  # Grayscale\n",
        "        im = ax.imshow(x_np, cmap='gray')\n",
        "    else:\n",
        "        im = ax.imshow(x_np)\n",
        "    plt.title(title)\n",
        "    ax.axis('off')\n",
        "    fig.set_size_inches(10, 10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fdJFcSO0DrMU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and extracting the dataset\n",
        "# Uncomment the following lines if you are running this in a Jupyter Notebook\n",
        "!wget https://gist.githubusercontent.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1/raw/403325f5110cb0f3099734c5edb9f457539c77e9/Oxford-102_Flower_dataset_labels.txt\n",
        "!wget https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "!unzip 'flower_data.zip'"
      ],
      "metadata": {
        "id": "9bIG_DcPxI4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56409539-f0b5-45df-ef5c-015e85220b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-19 15:22:42--  https://gist.githubusercontent.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1/raw/403325f5110cb0f3099734c5edb9f457539c77e9/Oxford-102_Flower_dataset_labels.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘Oxford-102_Flower_dataset_labels.txt.2’\n",
            "\n",
            "\r          Oxford-10   0%[                    ]       0  --.-KB/s               \rOxford-102_Flower_d 100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-19 15:22:42 (23.6 MB/s) - ‘Oxford-102_Flower_dataset_labels.txt.2’ saved [1608/1608]\n",
            "\n",
            "--2023-10-19 15:22:42--  https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.67.94, 16.182.64.176, 52.217.96.70, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.67.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311442766 (297M) [application/zip]\n",
            "Saving to: ‘flower_data.zip.2’\n",
            "\n",
            "flower_data.zip.2   100%[===================>] 297.01M  55.1MB/s    in 5.5s    \n",
            "\n",
            "2023-10-19 15:22:48 (53.7 MB/s) - ‘flower_data.zip.2’ saved [311442766/311442766]\n",
            "\n",
            "Archive:  flower_data.zip\n",
            "replace flower_data/valid/61/image_06296.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Directory and transforms\n",
        "data_dir = '/content/flower_data/'\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transform)\n",
        "dataset_labels = pd.read_csv('Oxford-102_Flower_dataset_labels.txt', header=None)[0].str.replace(\"'\", \"\").str.strip()\n",
        "\n",
        "# Load the dataset into a DataLoader for batching\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)"
      ],
      "metadata": {
        "id": "AKde_7VoCBB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the batch of images and labels\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "print(f\"Images tensor shape: {images.shape}\")\n",
        "print(f\"Labels tensor shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "id": "yNmdFAMjEOar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 55\n",
        "plot(images[i],dataset_labels[i]);"
      ],
      "metadata": {
        "id": "OEfJHtNqCOBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained Alexnet"
      ],
      "metadata": {
        "id": "1ilFoSaUa2R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#define alexnet model\n",
        "alexnet = models.alexnet(pretrained=True).to(device)\n",
        "labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()}\n",
        "\n",
        "#transform image for use in model\n",
        "preprocess = transforms.Compose([\n",
        "   transforms.Resize(256),\n",
        "   transforms.CenterCrop(224),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "Rtaj-tsed8iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[i]"
      ],
      "metadata": {
        "id": "SmetTIj6mDFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "to_pil = ToPILImage()\n",
        "img = to_pil(img)"
      ],
      "metadata": {
        "id": "7k8RESQsexB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t = preprocess(img).unsqueeze_(0).to(device)"
      ],
      "metadata": {
        "id": "XkTMbUhidqQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape"
      ],
      "metadata": {
        "id": "q2JJuEmCHMB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels"
      ],
      "metadata": {
        "id": "rmv1DMuuHXxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify the image with alexnet\n",
        "scores, class_idx = alexnet(img_t).max(1)\n",
        "print('Predicted class:', labels[class_idx.item()])"
      ],
      "metadata": {
        "id": "mgpHseEHHKSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = alexnet.features[0].weight.data\n",
        "w1 = alexnet.features[3].weight.data\n",
        "w2 = alexnet.features[6].weight.data\n",
        "w3 = alexnet.features[8].weight.data\n",
        "w4 = alexnet.features[10].weight.data\n",
        "w5 = alexnet.classifier[1].weight.data\n",
        "w6 = alexnet.classifier[4].weight.data\n",
        "w7 = alexnet.classifier[6].weight.data"
      ],
      "metadata": {
        "id": "I9jMV0nSvW-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and Load\n",
        "# w = [w0,w1,w2,w3,w4,w5,w6,w7]\n",
        "# torch.save(w, 'Hahn_Alex.pt')\n",
        "# w = torch.load('Hahn_Alex.pt')\n",
        "# [w0,w1,w2,w3,w4,w5,w6,w7] = w\n",
        "# [w0,w1,w2,w3,w4,w5,w6,w7] = torch.load('Hahn_Alex.pt')"
      ],
      "metadata": {
        "id": "Bo4sKueVvXBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape,w0.shape"
      ],
      "metadata": {
        "id": "NTghf1WV74of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape"
      ],
      "metadata": {
        "id": "YYbON_3XJr3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t[0,:,:,:].shape"
      ],
      "metadata": {
        "id": "Zh143pB2KAoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(img):\n",
        "    # Normalize the NumPy array to the range [0, 1]\n",
        "    max_value = img.max()\n",
        "    min_value = img.min()\n",
        "    normalized_array = (img - min_value) / (max_value - min_value)\n",
        "    return normalized_array"
      ],
      "metadata": {
        "id": "fupA6inAMMWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_plot(img_t,index=0):\n",
        "    numpy_array = img_t[index,:,:,:].cpu().numpy()\n",
        "    numpy_array_transposed = numpy_array.transpose(1, 2, 0)\n",
        "    numpy_array_transposed = scale(numpy_array_transposed)\n",
        "    plt.imshow(numpy_array_transposed)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "E45WF77_LNG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_plot(img_t)"
      ],
      "metadata": {
        "id": "MnLVNs_CLWnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0.shape"
      ],
      "metadata": {
        "id": "_GJy-69VLw53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f0 = F.conv2d(img_t, w0, stride=4, padding=2)"
      ],
      "metadata": {
        "id": "0dhT4aowvXEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f0.shape"
      ],
      "metadata": {
        "id": "iOIIu6zJHyjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.imshow(f0[0,i,:,:].cpu().numpy())"
      ],
      "metadata": {
        "id": "uFGvWefmNh43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(64):\n",
        "#     tensor_plot(w0,i)\n",
        "#     plt.imshow(f0[0,i,:,:].cpu().numpy())\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "-KwGtc8wM7DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_feature_maps_with_filters(feature_maps, filters):\n",
        "    # Remove batch dimension if it exists\n",
        "    if feature_maps.dim() == 4:\n",
        "        feature_maps = feature_maps.squeeze(0)\n",
        "\n",
        "    # Normalize feature maps to [0, 1]\n",
        "    feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())\n",
        "\n",
        "    def add_filter_to_feature_map(filter_tensor, feature_map_tensor):\n",
        "        # Ensure the feature map is 2D [H, W]\n",
        "        if feature_map_tensor.dim() > 2:\n",
        "            feature_map_tensor = feature_map_tensor.squeeze(0)\n",
        "\n",
        "        # Convert grayscale feature map to RGB by repeating the single channel 3 times\n",
        "        feature_map_rgb = feature_map_tensor.unsqueeze(0).repeat((3, 1, 1))\n",
        "\n",
        "        # Normalize the filter to [0, 1]\n",
        "        filter_tensor = (filter_tensor - filter_tensor.min()) / (filter_tensor.max() - filter_tensor.min())\n",
        "\n",
        "        # Ensure the filter fits into the feature map\n",
        "        min_dim = min(feature_map_tensor.shape)\n",
        "        filter_size = min(filter_tensor.shape[-1], min_dim)\n",
        "\n",
        "        # Crop the filter if needed\n",
        "        filter_cropped = filter_tensor[:, :filter_size, :filter_size]\n",
        "\n",
        "        # Overlay the RGB filter at the lower-left corner of the feature map\n",
        "        feature_map_rgb[:, -filter_size:, :filter_size] = filter_cropped\n",
        "\n",
        "        # Clip the values to be in the range [0, 1]\n",
        "        feature_map_rgb = torch.clamp(feature_map_rgb, 0, 1)\n",
        "\n",
        "        return feature_map_rgb\n",
        "\n",
        "    # Plot montage of feature maps\n",
        "    fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
        "\n",
        "    for ax, feature_map, filter_ in zip(axes.flat, feature_maps, filters):\n",
        "        # Add RGB filter to grayscale feature map\n",
        "        modified_feature_map = add_filter_to_feature_map(filter_, feature_map)\n",
        "\n",
        "        # Plot modified feature map\n",
        "        ax.imshow(modified_feature_map.permute(1, 2, 0).cpu().numpy(), interpolation='none')  # Added 'none' interpolation\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qkMqr835N-W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f0.shape,w0.shape"
      ],
      "metadata": {
        "id": "Ug6cetACrtzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_feature_maps_with_filters(f0, w0)"
      ],
      "metadata": {
        "id": "mQb-EZfpq-N8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}